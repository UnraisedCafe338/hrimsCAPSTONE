PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python311\python.exe C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py "hi"
RETURN: 1
OUTPUT:
The system cannot find the path specified.

PROMPT: hello
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python311\python.exe C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py "hello"
RETURN: 1
OUTPUT:
The system cannot find the path specified.

PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python311\python.exe ai_script.py "hi"
RETURN: 1
OUTPUT:
The system cannot find the path specified.

PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python311\python.exe ai_script.py "hi"
RETURN: 1
OUTPUT:
The system cannot find the path specified.

PROMPT: hi
COMMAND: python ai_script.py "hi"
RETURN: 1
OUTPUT:
'python' is not recognized as an internal or external command,
operable program or batch file.

PROMPT: hi
COMMAND: python3 ai_script.py "hi"
RETURN: 1
OUTPUT:
'python3' is not recognized as an internal or external command,
operable program or batch file.

PROMPT: hi
COMMAND: python3 ai_script.py "hi"
RETURN: 1
OUTPUT:
'python3' is not recognized as an internal or external command,
operable program or batch file.

PROMPT: hi
COMMAND: python3 ai_script.py "hi"
RETURN: 1
OUTPUT:
'python3' is not recognized as an internal or external command,
operable program or batch file.

PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "hi"
RETURN: 0
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Hello! How can I help you today? If you have any questions or need assistance with something, feel free to ask. I'm here to help. If you just wanted to say hi, then hi back! :)

PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "hi"
RETURN: 0
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Hello! How can I help you today? If you have any questions or need assistance with something, feel free to ask. I'm here to help. If you just wanted to say hi, then hi back! :)

PROMPT: read the document content pls
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "read the document content pls"
RETURN: 0
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
I'm sorry, but there is no document provided for me to read. Could you please attach or share the document with me so I can help you with any questions or tasks related to its content?

PROMPT: can you analyze the content of that file
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "can you analyze the content of that file"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 41, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 34, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 15, in get_ai_response
    resume_content = read_latest_docx("uploads")
                     ^^^^^^^^^^^^^^^^
NameError: name 'read_latest_docx' is not defined

PROMPT: can you analyze the content of that file
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "can you analyze the content of that file"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 62, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 54, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 33, in get_ai_response
    resume_content = read_latest_docx("uploads")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 8, in read_latest_docx
    docx_files = [f for f in os.listdir(upload_dir) if f.endswith(".docx")]
                             ^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'uploads'

PROMPT: read the file
COMMAND: "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe" "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py" "read the file"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 62, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 54, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 33, in get_ai_response
    resume_content = read_latest_docx("uploads")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 8, in read_latest_docx
    docx_files = [f for f in os.listdir(upload_dir) if f.endswith(".docx")]
                             ^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'uploads'

PROMPT: hi
COMMAND: "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe" "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py" "hi"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 62, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 54, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 33, in get_ai_response
    resume_content = read_latest_docx("uploads")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 8, in read_latest_docx
    docx_files = [f for f in os.listdir(upload_dir) if f.endswith(".docx")]
                             ^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'uploads'

STARTED PHP SCRIPT
PROMPT RECEIVED: hi
COMMAND: "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe" "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py" "hi"
RETURN VAR: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 62, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 54, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 33, in get_ai_response
    resume_content = read_latest_docx("uploads")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 8, in read_latest_docx
    docx_files = [f for f in os.listdir(upload_dir) if f.endswith(".docx")]
                             ^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'uploads'

STARTED PHP SCRIPT
PROMPT RECEIVED: hi
COMMAND: "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe" "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py" "hi"
RETURN VAR: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 62, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 54, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 33, in get_ai_response
    resume_content = read_latest_docx("uploads")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 8, in read_latest_docx
    docx_files = [f for f in os.listdir(upload_dir) if f.endswith(".docx")]
                             ^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'uploads'

STARTED PHP SCRIPT
PROMPT RECEIVED: bruh
COMMAND: "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe" "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py" "bruh"
RETURN VAR: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 62, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 54, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 33, in get_ai_response
    resume_content = read_latest_docx("uploads")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu/ai_script.py", line 8, in read_latest_docx
    docx_files = [f for f in os.listdir(upload_dir) if f.endswith(".docx")]
                             ^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'uploads'

PROMPT: hello
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "hello"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 62, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 54, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 33, in get_ai_response
    resume_content = read_latest_docx("uploads")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 8, in read_latest_docx
    docx_files = [f for f in os.listdir(upload_dir) if f.endswith(".docx")]
                             ^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'uploads'

PROMPT: hey
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "hey"
RETURN: 0
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Hello! How can I help you today? If you have any questions or need assistance with something, feel free to ask. I'm here to help. If you just want to chat or share some thoughts, I'd be happy to listen as well. Let me know what's on your mind.

PROMPT: read that document content
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "read that document content"
RETURN: 0
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
I'm unable to read a document without being provided with the document itself. Please attach or share the document content with me, and I'll be happy to help you read and understand it.

PROMPT: read that document content
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "read that document content"
RETURN: 0
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
I'm unable to read a document without being provided with the document itself. Please attach or share the document content with me, and I'll be happy to help you read and understand it.

PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "hi"
RETURN: 0
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Hi there! It looks like the resume you've provided is a list of various beverages and some brands. If you have a specific question related to this list or if you'd like to ask about something entirely different, feel free to let me know!

If you're asking about the beverages listed, here's a brief overview of each one:

* Coke (Maliit) and Coke (Kasalo): These are likely local variations or sizes of Coca-Cola.
* Royal (Maliit) and Royal (Kasalo): These could be local brands or sizes of cola or soda.
* Sprite (Maliit) and Sprite (Kasalo): Similar to the previous entries, these are likely local variations or sizes of Sprite.
* Royal Lemon (Maliit): A local variation or size of lemon soda.
* C2 (Apple, Lemon, Dalandan): A beverage with three flavors - apple, lemon, and dalandan.
* Mountain Dew Pet Bottle: A larger-sized bottle of Mountain Dew.
* Cobra and Sting: Brands of energy drinks.
* Purified Water (330ML): Bottled water in a 330ml size.
* Zesto Juice: A brand of fresh juice.
* Minute

PROMPT: how about this
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "how about this"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 45, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 39, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 32, in get_ai_response
    response = llm(prompt, max_tokens=300, stop=["</s>"])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1902, in __call__
    return self.create_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1835, in create_completion
    completion: Completion = next(completion_or_chunks)  # type: ignore
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1271, in _create_completion
    raise ValueError(
ValueError: Requested tokens (3645) exceed context window of 2048

PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "hi"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 45, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 39, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 32, in get_ai_response
    response = llm(prompt, max_tokens=300, stop=["</s>"])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1902, in __call__
    return self.create_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1835, in create_completion
    completion: Completion = next(completion_or_chunks)  # type: ignore
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1271, in _create_completion
    raise ValueError(
ValueError: Requested tokens (3643) exceed context window of 2048

PROMPT: read it
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "read it"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 45, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 39, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 32, in get_ai_response
    response = llm(prompt, max_tokens=300, stop=["</s>"])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1902, in __call__
    return self.create_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1835, in create_completion
    completion: Completion = next(completion_or_chunks)  # type: ignore
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1271, in _create_completion
    raise ValueError(
ValueError: Requested tokens (3644) exceed context window of 2048

PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "hi"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 45, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 39, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 32, in get_ai_response
    response = llm(prompt, max_tokens=300, stop=["</s>"])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1902, in __call__
    return self.create_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1835, in create_completion
    completion: Completion = next(completion_or_chunks)  # type: ignore
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1271, in _create_completion
    raise ValueError(
ValueError: Requested tokens (3643) exceed context window of 2048

PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "hi"
RETURN: 1
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
Traceback (most recent call last):
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 45, in <module>
    process_command_line_input()
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 39, in process_command_line_input
    ai_response = get_ai_response(user_input)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py", line 32, in get_ai_response
    response = llm(prompt, max_tokens=300, stop=["</s>"])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1902, in __call__
    return self.create_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1835, in create_completion
    completion: Completion = next(completion_or_chunks)  # type: ignore
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\Lib\site-packages\llama_cpp\llama.py", line 1271, in _create_completion
    raise ValueError(
ValueError: Requested tokens (3643) exceed context window of 2048

====== AI DEBUG ======
TIME: 2025-04-17 05:56:58
PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py "hi"
RETURN: 0
OUTPUT:
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:/Users/LENOVO/Downloads/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW)
init_tokenizer: initializing tokenizer for type 1
load: control token:      2 '</s>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.24 B
print_info: general.name     = mistralai_mistral-7b-instruct-v0.2
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CPU
load_tensors: layer   1 assigned to device CPU
load_tensors: layer   2 assigned to device CPU
load_tensors: layer   3 assigned to device CPU
load_tensors: layer   4 assigned to device CPU
load_tensors: layer   5 assigned to device CPU
load_tensors: layer   6 assigned to device CPU
load_tensors: layer   7 assigned to device CPU
load_tensors: layer   8 assigned to device CPU
load_tensors: layer   9 assigned to device CPU
load_tensors: layer  10 assigned to device CPU
load_tensors: layer  11 assigned to device CPU
load_tensors: layer  12 assigned to device CPU
load_tensors: layer  13 assigned to device CPU
load_tensors: layer  14 assigned to device CPU
load_tensors: layer  15 assigned to device CPU
load_tensors: layer  16 assigned to device CPU
load_tensors: layer  17 assigned to device CPU
load_tensors: layer  18 assigned to device CPU
load_tensors: layer  19 assigned to device CPU
load_tensors: layer  20 assigned to device CPU
load_tensors: layer  21 assigned to device CPU
load_tensors: layer  22 assigned to device CPU
load_tensors: layer  23 assigned to device CPU
load_tensors: layer  24 assigned to device CPU
load_tensors: layer  25 assigned to device CPU
load_tensors: layer  26 assigned to device CPU
load_tensors: layer  27 assigned to device CPU
load_tensors: layer  28 assigned to device CPU
load_tensors: layer  29 assigned to device CPU
load_tensors: layer  30 assigned to device CPU
load_tensors: layer  31 assigned to device CPU
load_tensors: layer  32 assigned to device CPU
load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB
.................................................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 1024
llama_init_from_model: n_ctx_per_seq = 1024
llama_init_from_model: n_batch       = 512
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 1000000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 1024, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB
llama_init_from_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.12 MiB
llama_init_from_model:        CPU compute buffer size =    98.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 1
CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |
Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': "{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"}
Available chat formats from metadata: chat_template.default
Guessed chat format: mistral-instruct
llama_perf_context_print:        load time =   10334.38 ms
llama_perf_context_print: prompt eval time =   10333.26 ms /    43 tokens (  240.31 ms per token,     4.16 tokens per second)
llama_perf_context_print:        eval time =   56152.86 ms /   199 runs   (  282.18 ms per token,     3.54 tokens per second)
llama_perf_context_print:       total time =   66966.69 ms /   242 tokens
Hello! I see you've encountered an error message while trying to access a file path 'uploads' on your system. This error typically occurs when the specified directory does not exist or cannot be accessed. Here are some steps you can take to troubleshoot this issue:

1. Check if the directory 'uploads' exists: You can do this by opening File Explorer and navigating to the location where you think the directory should be. If the directory is not there, you may need to create it.

2. Create the 'uploads' directory: If the directory does not exist, you can create it by following these steps:

   a. Open File Explorer and go to the location where you want to create the new directory.
   b. Right-click on an empty area and select "New" > "Folder."
   c. Name the new folder "uploads" (without quotes).

3. Check if

====== AI DEBUG ======
TIME: 2025-04-17 08:25:58
PROMPT: hi AI
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py "hi AI"
RETURN: 0
OUTPUT:
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:/Users/LENOVO/Downloads/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW)
init_tokenizer: initializing tokenizer for type 1
load: control token:      2 '</s>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.24 B
print_info: general.name     = mistralai_mistral-7b-instruct-v0.2
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CPU
load_tensors: layer   1 assigned to device CPU
load_tensors: layer   2 assigned to device CPU
load_tensors: layer   3 assigned to device CPU
load_tensors: layer   4 assigned to device CPU
load_tensors: layer   5 assigned to device CPU
load_tensors: layer   6 assigned to device CPU
load_tensors: layer   7 assigned to device CPU
load_tensors: layer   8 assigned to device CPU
load_tensors: layer   9 assigned to device CPU
load_tensors: layer  10 assigned to device CPU
load_tensors: layer  11 assigned to device CPU
load_tensors: layer  12 assigned to device CPU
load_tensors: layer  13 assigned to device CPU
load_tensors: layer  14 assigned to device CPU
load_tensors: layer  15 assigned to device CPU
load_tensors: layer  16 assigned to device CPU
load_tensors: layer  17 assigned to device CPU
load_tensors: layer  18 assigned to device CPU
load_tensors: layer  19 assigned to device CPU
load_tensors: layer  20 assigned to device CPU
load_tensors: layer  21 assigned to device CPU
load_tensors: layer  22 assigned to device CPU
load_tensors: layer  23 assigned to device CPU
load_tensors: layer  24 assigned to device CPU
load_tensors: layer  25 assigned to device CPU
load_tensors: layer  26 assigned to device CPU
load_tensors: layer  27 assigned to device CPU
load_tensors: layer  28 assigned to device CPU
load_tensors: layer  29 assigned to device CPU
load_tensors: layer  30 assigned to device CPU
load_tensors: layer  31 assigned to device CPU
load_tensors: layer  32 assigned to device CPU
load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB
.................................................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 1024
llama_init_from_model: n_ctx_per_seq = 1024
llama_init_from_model: n_batch       = 512
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 1000000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 1024, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB
llama_init_from_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.12 MiB
llama_init_from_model:        CPU compute buffer size =    98.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 1
CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |
Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': "{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"}
Available chat formats from metadata: chat_template.default
Guessed chat format: mistral-instruct
llama_perf_context_print:        load time =   11005.02 ms
llama_perf_context_print: prompt eval time =   11002.77 ms /    44 tokens (  250.06 ms per token,     4.00 tokens per second)
llama_perf_context_print:        eval time =   78288.43 ms /   199 runs   (  393.41 ms per token,     2.54 tokens per second)
llama_perf_context_print:       total time =   89660.69 ms /   243 tokens
Hello! I see you've encountered an error message while trying to access a file path 'uploads' on your system. This error typically occurs when the specified directory does not exist or cannot be accessed. Here are some steps you can take to troubleshoot this issue:

1. Check if the directory 'uploads' exists: Open File Explorer and navigate to the directory location. If the directory is not present, you can create it by right-clicking on the desired folder, selecting "New", and then "Folder". Name the folder "uploads" and try to access it again.

2. Check if you have the necessary permissions: Make sure that your user account has read and write permissions for the 'uploads' directory. You can check this by right-clicking on the folder, selecting "Properties", and then navigating to the "Security" tab.

3. Re-register the associated application: If the error is occurring

====== AI DEBUG ======
TIME: 2025-04-17 08:50:46
PROMPT: hi
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe C:\xampp\htdocs\hrims\sidebar_menu\ai_script.py "hi"
RETURN: 0
OUTPUT:
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:/Users/LENOVO/Downloads/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW)
init_tokenizer: initializing tokenizer for type 1
load: control token:      2 '</s>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 7.24 B
print_info: general.name     = mistralai_mistral-7b-instruct-v0.2
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CPU
load_tensors: layer   1 assigned to device CPU
load_tensors: layer   2 assigned to device CPU
load_tensors: layer   3 assigned to device CPU
load_tensors: layer   4 assigned to device CPU
load_tensors: layer   5 assigned to device CPU
load_tensors: layer   6 assigned to device CPU
load_tensors: layer   7 assigned to device CPU
load_tensors: layer   8 assigned to device CPU
load_tensors: layer   9 assigned to device CPU
load_tensors: layer  10 assigned to device CPU
load_tensors: layer  11 assigned to device CPU
load_tensors: layer  12 assigned to device CPU
load_tensors: layer  13 assigned to device CPU
load_tensors: layer  14 assigned to device CPU
load_tensors: layer  15 assigned to device CPU
load_tensors: layer  16 assigned to device CPU
load_tensors: layer  17 assigned to device CPU
load_tensors: layer  18 assigned to device CPU
load_tensors: layer  19 assigned to device CPU
load_tensors: layer  20 assigned to device CPU
load_tensors: layer  21 assigned to device CPU
load_tensors: layer  22 assigned to device CPU
load_tensors: layer  23 assigned to device CPU
load_tensors: layer  24 assigned to device CPU
load_tensors: layer  25 assigned to device CPU
load_tensors: layer  26 assigned to device CPU
load_tensors: layer  27 assigned to device CPU
load_tensors: layer  28 assigned to device CPU
load_tensors: layer  29 assigned to device CPU
load_tensors: layer  30 assigned to device CPU
load_tensors: layer  31 assigned to device CPU
load_tensors: layer  32 assigned to device CPU
load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB
.................................................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 1024
llama_init_from_model: n_ctx_per_seq = 1024
llama_init_from_model: n_batch       = 512
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 1000000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 1024, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB
llama_init_from_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.12 MiB
llama_init_from_model:        CPU compute buffer size =    98.01 MiB
llama_init_from_model: graph nodes  = 1030
llama_init_from_model: graph splits = 1
CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |
Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': "{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"}
Available chat formats from metadata: chat_template.default
Guessed chat format: mistral-instruct
llama_perf_context_print:        load time =    9251.21 ms
llama_perf_context_print: prompt eval time =    9250.57 ms /    43 tokens (  215.13 ms per token,     4.65 tokens per second)
llama_perf_context_print:        eval time =   74527.68 ms /   199 runs   (  374.51 ms per token,     2.67 tokens per second)
llama_perf_context_print:       total time =   84435.67 ms /   242 tokens
Hello! I see you've encountered an error message while trying to access a file path 'uploads' on your system. This error typically occurs when the specified directory does not exist or cannot be accessed. Here are some steps you can take to troubleshoot this issue:

1. Check if the directory 'uploads' exists: You can do this by opening File Explorer and navigating to the location where you think the directory should be. If the directory is not there, you may need to create it.

2. Create the 'uploads' directory: If the directory does not exist, you can create it by following these steps:

   a. Open File Explorer and go to the location where you want to create the new directory.
   b. Right-click on an empty area and select "New" > "Folder."
   c. Name the new folder "uploads" (without quotes).

3. Check if

PROMPT: read the file
FILE: AVAILABLE_HERE.docx
COMMAND: C:\Users\LENOVO\AppData\Local\Programs\Python\Python312\python.exe ai_script.py "read the file" AVAILABLE_HERE.docx
RETURN: 0
OUTPUT:
llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
I'm sorry, but there seems to be a misunderstanding. I cannot read a resume file if none has been provided. Please attach or share the file with me, and I'll be happy to help you review it.

